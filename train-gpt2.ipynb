{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb2kt1SjefJW",
        "outputId": "c882dc8b-4ecf-48dd-b6d4-5209aa85d0ad"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers wandb tiktoken\n",
        "!sudo apt-get install unzip\n",
        "%pwd\n",
        "%cd workspace\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!unzip /workspace/fineweb-25k.zip -d /workspace/dataset-25K/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!unzip /workspace/fineweb-ref.zip -d /workspace/dataset-ref/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fx56J0F1Y-0"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tVtwU_PZWxN6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import wandb\n",
        "import tiktoken\n",
        "\n",
        "TRAIN_SPACE = True\n",
        "data_root = \"dataset-25K/content/data/\" if TRAIN_SPACE else \"dataset-ref/content/data/\"\n",
        "\n",
        "total_batch_size = 491520 # 524288 # 2**19, ~0.5M, in number of tokens\n",
        "B = 96 if TRAIN_SPACE else 80 # 64 # micro batch size # 64\n",
        "T = 1024 # sequence length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('wandb.txt', 'r') as file:\n",
        "    wandb_key = file.read()\n",
        "wandb.login(key=wandb_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EgeEDiIT1YPu"
      },
      "outputs": [],
      "source": [
        "# From https://github.com/karpathy/build-nanogpt/blob/master/train_gpt2.py\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 12 # number of layers\n",
        "    n_head: int = 12 # number of heads\n",
        "    n_embd: int = 768 # embedding dimension\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        if TRAIN_SPACE:\n",
        "            self.transformer = nn.ModuleDict(dict(\n",
        "                wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "                h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "                ln_f = nn.LayerNorm(config.n_embd),\n",
        "            ))\n",
        "        else:\n",
        "            self.transformer = nn.ModuleDict(dict(\n",
        "                wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "                wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "                h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "                ln_f = nn.LayerNorm(config.n_embd),\n",
        "            ))\n",
        "\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        if TRAIN_SPACE:\n",
        "            self.emb_ids = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "            self.emb_space = nn.Embedding(2, config.n_embd)\n",
        "            self.emb_case = nn.Embedding(2, config.n_embd)\n",
        "\n",
        "            self.lm_head_space = nn.Linear(config.n_embd, 2, bias=False)\n",
        "            self.lm_head_case = nn.Linear(config.n_embd, 2, bias=False)\n",
        "\n",
        "            self.emb_ids.weight = self.lm_head.weight\n",
        "            self.emb_space.weight = self.lm_head_space.weight\n",
        "            self.emb_case.weight = self.lm_head_case.weight\n",
        "        else:\n",
        "            # weight sharing scheme\n",
        "            self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "\n",
        "        # init params\n",
        "        self.apply(self._init_weights)\n",
        "        if TRAIN_SPACE:\n",
        "            torch.nn.init.normal_(self.emb_ids.weight, mean=0.0, std=0.02)\n",
        "            torch.nn.init.normal_(self.emb_space.weight, mean=0.0, std=0.0000002)\n",
        "            torch.nn.init.normal_(self.emb_case.weight, mean=0.0, std=0.0000002)\n",
        "\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "\n",
        "        if TRAIN_SPACE:\n",
        "            # token embeddings of shape (B, T, n_embd)\n",
        "            ids, space, upper = self.unpack_token(idx)\n",
        "            tok_emb = self.emb_ids(ids) + self.emb_space(space) + self.emb_case(upper)\n",
        "        else:\n",
        "            tok_emb = self.transformer.wte(idx)\n",
        "\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        logits = self.lm_head(x)\n",
        "        if TRAIN_SPACE:\n",
        "            logits_space = self.lm_head_space(x)\n",
        "            logits_case = self.lm_head_case(x)\n",
        "        else:\n",
        "            logits_space = torch.zeros((B, T, 2))\n",
        "            logits_case = torch.zeros((B, T, 2))\n",
        "\n",
        "        loss_out = None\n",
        "        if targets is not None:\n",
        "            if TRAIN_SPACE:\n",
        "                targets_ids, targets_space, targets_upper = self.unpack_token(targets)\n",
        "\n",
        "                loss_ids = F.cross_entropy(logits.view(-1, logits.size(-1)), targets_ids.view(-1))\n",
        "                loss_space = F.cross_entropy(logits_space.view(-1, logits_space.size(-1)), targets_space.view(-1))\n",
        "                loss_case = F.cross_entropy(logits_case.view(-1, logits_case.size(-1)), targets_upper.view(-1))\n",
        "\n",
        "                loss = loss_ids + 0.1 * loss_space + 0.1 * loss_case\n",
        "                loss_out = (loss, loss_ids, loss_space, loss_case)\n",
        "            else:\n",
        "                loss_ids = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "                loss_out = (loss_ids, loss_ids, torch.tensor(0), torch.tensor(0))\n",
        "\n",
        "        return (logits, logits_space, logits_case), loss_out\n",
        "\n",
        "    def unpack_token(self, token):\n",
        "        id = token >> 2\n",
        "        space = (token >> 1) & 0x01\n",
        "        upper = (token >> 0) & 0x01\n",
        "        return id, space, upper\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # ignore the ones we don't have\n",
        "        new_keys = ['emb_space.weight', 'emb_case.weight', 'lm_head_space.weight', 'lm_head_case.weight', 'emb_ids.weight']\n",
        "        sd_keys = [k for k in sd_keys if not k in new_keys]\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "        # start with all of the candidate parameters (that require grad)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        if master_process:\n",
        "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "        if master_process:\n",
        "            print(f\"using fused AdamW: {use_fused}\")\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dVtHSDPmYTaq"
      },
      "outputs": [],
      "source": [
        "def load_tokens(filename):\n",
        "    npt = np.load(filename)\n",
        "    npt = npt.astype(np.int32) # added after video\n",
        "    ptt = torch.tensor(npt, dtype=torch.long)\n",
        "    return ptt\n",
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T, process_rank, num_processes, split):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "        self.process_rank = process_rank\n",
        "        self.num_processes = num_processes\n",
        "        assert split in {'train', 'val'}\n",
        "\n",
        "        # get the shard filenames\n",
        "        shards = os.listdir(data_root)\n",
        "        shards = [s for s in shards if split in s]\n",
        "        shards = sorted(shards)\n",
        "        shards = [os.path.join(data_root, s) for s in shards]\n",
        "        self.shards = shards\n",
        "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
        "        if master_process:\n",
        "            print(f\"found {len(shards)} shards for split {split}\")\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # state, init at shard zero\n",
        "        self.current_shard = 0\n",
        "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
        "        self.current_position = self.B * self.T * self.process_rank\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "        x = (buf[:-1]).view(B, T) # inputs\n",
        "        y = (buf[1:]).view(B, T) # targets\n",
        "        # advance the position in the tensor\n",
        "        self.current_position += B * T * self.num_processes\n",
        "        # if loading the next batch would be out of bounds, advance to next shard\n",
        "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
        "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
        "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
        "            self.current_position = B * T * self.process_rank\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f14AnIf6a8SL",
        "outputId": "a63b2dd9-abb2-4aa0-b347-4569f5e15b89"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# simple launch:\n",
        "# python train_gpt2.py\n",
        "# DDP launch for e.g. 8 GPUs:\n",
        "# torchrun --standalone --nproc_per_node=8 train_gpt2.py\n",
        "\n",
        "# run the training loop\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.distributed as dist\n",
        "\n",
        "# set up DDP (distributed data parallel).\n",
        "# torchrun command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
        "    assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n",
        "    init_process_group(backend='nccl')\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "else:\n",
        "    # vanilla, non-DDP run\n",
        "    ddp_rank = 0\n",
        "    ddp_local_rank = 0\n",
        "    ddp_world_size = 1\n",
        "    master_process = True\n",
        "    # attempt to autodetect device\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "    print(f\"using device: {device}\")\n",
        "\n",
        "# added after video, pytorch can be serious about it's device vs. device_type distinction\n",
        "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP854NSoauvd",
        "outputId": "e95594d9-4db3-4d36-c308-2a155699725e"
      },
      "outputs": [],
      "source": [
        "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
        "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
        "if master_process:\n",
        "    print(f\"total desired batch size: {total_batch_size}\")\n",
        "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"train\")\n",
        "val_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"val\")\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# create model\n",
        "vocab_size = (25000 + 257) if TRAIN_SPACE else 50257\n",
        "model = GPT(GPTConfig(vocab_size=vocab_size))\n",
        "# model = GPT.from_pretrained(\"gpt2\") # or init from OpenAI GPT-2\n",
        "model.to(device)\n",
        "use_compile = False # torch.compile interferes with HellaSwag eval and Generation. TODO fix\n",
        "if use_compile:\n",
        "    model = torch.compile(model)\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "arb28DDrrKDM"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "class TrieNode:\n",
        "    def __init__(self):\n",
        "        self.children = {}\n",
        "        self.token_id = None\n",
        "        self.token = None\n",
        "\n",
        "class Trie:\n",
        "    def __init__(self):\n",
        "        self.root = TrieNode()\n",
        "\n",
        "    def insert(self, token, token_id):\n",
        "        node = self.root\n",
        "        for char in token:\n",
        "            if char not in node.children:\n",
        "                node.children[char] = TrieNode()\n",
        "            node = node.children[char]\n",
        "        node.token_id = token_id\n",
        "        node.token = token\n",
        "\n",
        "    def search(self, text, start_pos):\n",
        "        match_token, match_token_id = None, None\n",
        "        pos = start_pos\n",
        "        node = self.root\n",
        "        while True:\n",
        "            char = text[pos]\n",
        "            if char not in node.children:\n",
        "                break\n",
        "            node = node.children[char]\n",
        "            if node.token:\n",
        "                match_token = node.token\n",
        "                match_token_id = node.token_id\n",
        "            pos += 1\n",
        "            if pos >= len(text):\n",
        "                break\n",
        "        return match_token_id, match_token\n",
        "\n",
        "def bytes_to_unicode():\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "\n",
        "def pack_token(id, space, upper):\n",
        "    return (id << 2) + (space << 1) + (upper << 0)\n",
        "\n",
        "def upper_first(text):\n",
        "    return text[0].upper() + (text[1:] if len(text) > 1 else \"\")\n",
        "\n",
        "def expand_vocab(vocab, max_vocab_size):\n",
        "    updated_vocab = {}\n",
        "    for i, (token, id) in enumerate(vocab.items()):\n",
        "        if i >= max_vocab_size:\n",
        "            return updated_vocab\n",
        "        updated_vocab[pack_token(id, space=False, upper=True)] = f\"{upper_first(token)}\"\n",
        "        updated_vocab[pack_token(id, space=True, upper=True)] = f\"Ġ{upper_first(token)}\"\n",
        "        updated_vocab[pack_token(id, space=False, upper=False)] = f\"{token}\"\n",
        "        updated_vocab[pack_token(id, space=True, upper=False)] = f\"Ġ{token}\"\n",
        "    return updated_vocab\n",
        "\n",
        "\n",
        "class SpaceTokenizer():\n",
        "    def __init__(self, vocab_config, vocab_size=None):\n",
        "      self.byte_encoder = bytes_to_unicode()\n",
        "      self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
        "\n",
        "      vocab_size = len(vocab_config) if vocab_size is None else vocab_size\n",
        "      self.vocab_decode = expand_vocab(vocab_config, max_vocab_size=vocab_size)\n",
        "      self.vocab = {v:k for k,v in self.vocab_decode.items()}\n",
        "\n",
        "      self.trie = Trie()\n",
        "      for token, token_id in self.vocab.items():\n",
        "          self.trie.insert(token, token_id)\n",
        "\n",
        "    def encode(self, text, return_token_tuple=False):\n",
        "        text = ''.join(self.byte_encoder[b] for b in text.encode('utf-8'))\n",
        "        pos = 0\n",
        "        ids, tokens = [], []\n",
        "        while True:\n",
        "            id, token = self.trie.search(text, pos)\n",
        "            if id is None or token is None:\n",
        "                raise Exception(f\"Error encoding {text[pos:pos+16]}\")\n",
        "            ids.append(id)\n",
        "            tokens.append(token)\n",
        "            pos += len(token)\n",
        "            if pos >= len(text):\n",
        "                break\n",
        "        return (ids, tokens) if return_token_tuple else ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        out = \"\"\n",
        "        for id in ids:\n",
        "            if not id in self.vocab_decode:\n",
        "                raise Exception(f\"Error decoding {id}\")\n",
        "            out += self.vocab_decode[id]\n",
        "        return bytearray([self.byte_decoder[c] for c in out]).decode('utf-8', errors=\"replace\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trGyyzHRlBR_",
        "outputId": "4594dd76-5c90-4a91-a629-1e85b40e0962"
      },
      "outputs": [],
      "source": [
        "x, y = val_loader.next_batch()\n",
        "print(x.shape, y.shape)\n",
        "\n",
        "x, y = train_loader.next_batch()\n",
        "x, y = x.to(device), y.to(device)\n",
        "with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "    logits, loss = model(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "IxI_5kbJrR2w",
        "outputId": "3b9b66ed-26b9-454a-fa9b-6b2684d97ab2"
      },
      "outputs": [],
      "source": [
        "with open('./tokenizer-space.json', 'r', encoding='utf-8') as f: tokenizer_config = json.load(f)\n",
        "\n",
        "vocab = tokenizer_config[\"model\"][\"vocab\"]\n",
        "# vocab_size = 25000 + 257 # set previously\n",
        "\n",
        "tokenizer = SpaceTokenizer(vocab, vocab_size)\n",
        "\n",
        "if not TRAIN_SPACE:\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "tokenizer.decode(list(x[0].cpu().tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oJctlHW0YMya",
        "outputId": "5c76fe96-295e-41c4-b8d1-ec12a6a1567f"
      },
      "outputs": [],
      "source": [
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 500 # 715\n",
        "max_steps = 5000 # 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "# optimize!\n",
        "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device_type)\n",
        "\n",
        "project_name = f\"x-{'25K' if vocab_size < 26000 else '50K'}\" if TRAIN_SPACE else \"x-ref\"\n",
        "run = wandb.init(\n",
        "    project=\"space-gpt\",\n",
        "    name=project_name,\n",
        "    config={\n",
        "        \"vocab_size\": vocab_size,\n",
        "        \"max_lr\": max_lr,\n",
        "        \"min_lr\": min_lr,\n",
        "        \"warmup_steps\": warmup_steps,\n",
        "        \"max_steps\": max_steps,\n",
        "        \"total_batch_size\": total_batch_size,\n",
        "        \"B\": B,\n",
        "        \"T\": T\n",
        "    },\n",
        ")\n",
        "\n",
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    last_step = (step == max_steps - 1)\n",
        "\n",
        "    # once in a while evaluate our validation loss\n",
        "    if step % 50 == 0 or last_step:\n",
        "        model.eval()\n",
        "        val_loader.reset()\n",
        "        with torch.no_grad():\n",
        "            val_loss_accum, val_loss_accum_ids, val_loss_accum_space, val_loss_accum_case = 0.0, 0.0, 0.0, 0.0\n",
        "            val_loss_steps = 20\n",
        "            for _ in range(val_loss_steps):\n",
        "                x, y = val_loader.next_batch()\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "                    logits, losses = model(x, y)\n",
        "                (loss, loss_ids, loss_space, loss_case) = losses\n",
        "                val_loss_accum += (loss / val_loss_steps).detach()\n",
        "                val_loss_accum_ids += (loss_ids / val_loss_steps).detach()\n",
        "                val_loss_accum_space += (loss_space / val_loss_steps).detach()\n",
        "                val_loss_accum_case += (loss_case / val_loss_steps).detach()\n",
        "        if ddp:\n",
        "            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n",
        "            dist.all_reduce(val_loss_accum_ids, op=dist.ReduceOp.AVG)\n",
        "            dist.all_reduce(val_loss_accum_space, op=dist.ReduceOp.AVG)\n",
        "            dist.all_reduce(val_loss_accum_case, op=dist.ReduceOp.AVG)\n",
        "        if master_process:\n",
        "            print(f\"validation loss: {val_loss_accum.item():.4f}, ids: {val_loss_accum_ids.item():.4f}, space: {val_loss_accum_space.item():.4f}, case: {val_loss_accum_case.item():.4f}\")\n",
        "            run.log({\"step\": step, \"val/loss\": val_loss_accum.item(), \"val/loss_ids\": val_loss_accum_ids.item(), \"val/loss_space\": val_loss_accum_space.item(), \"val/loss_case\": val_loss_accum_case.item()})\n",
        "\n",
        "            if step > 0 and (step % 1000 == 0 or last_step):\n",
        "                # optionally write model checkpoints\n",
        "                checkpoint_path = f\"model_{step:05d}.pt\"\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'config': raw_model.config,\n",
        "                    'step': step,\n",
        "                    'val_loss': val_loss_accum.item()\n",
        "                }\n",
        "                # you might also want to add optimizer.state_dict() and\n",
        "                # rng seeds etc., if you wanted to more exactly resume training\n",
        "                torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "                artifact = wandb.Artifact(name=f\"{project_name}-model\", type=\"model\")\n",
        "                artifact.add_file(local_path=checkpoint_path)\n",
        "                run.log_artifact(artifact)\n",
        "\n",
        "\n",
        "    # once in a while generate from the model (except step 0, which is noise)\n",
        "    if ((step > 0 and step % 100 == 0) or last_step) and (not use_compile):\n",
        "        def pack_token(id, space, upper):\n",
        "            return (id << 2) + (space << 1) + (upper << 0)\n",
        "        samples = []\n",
        "\n",
        "        model.eval()\n",
        "        num_return_sequences = 4\n",
        "        max_length = 32\n",
        "        tokens = tokenizer.encode(\"Hello, I'm a language model,\")\n",
        "\n",
        "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "        tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "        xgen = tokens.to(device)\n",
        "        sample_rng = torch.Generator(device=device)\n",
        "        sample_rng.manual_seed(42 + ddp_rank)\n",
        "        while xgen.size(1) < max_length:\n",
        "            # forward the model to get the logits\n",
        "            with torch.no_grad():\n",
        "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "                    logits, _ = model(xgen) # (B, T, vocab_size)\n",
        "                # take the logits at the last position\n",
        "                if TRAIN_SPACE:\n",
        "                    logits_id, logits_space, logits_case = logits\n",
        "                    logits_id = logits_id[:, -1, :] # (B, vocab_size)\n",
        "                    logits_space = logits_space[:, -1, :] # (B, vocab_size)\n",
        "                    logits_case = logits_case[:, -1, :] # (B, vocab_size)\n",
        "                    #print(logits_id.shape, logits_space.shape)\n",
        "                    space = torch.argmax(logits_space, dim=-1).unsqueeze(1)\n",
        "                    upper = torch.argmax(logits_case, dim=-1).unsqueeze(1)\n",
        "                else:\n",
        "                    logits_id = logits[0][:, -1, :]\n",
        "\n",
        "                # get the probabilities\n",
        "                probs = F.softmax(logits_id, dim=-1)\n",
        "                # do top-k sampling of 50 (huggingface pipeline default)\n",
        "                # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "                topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "                # select a token from the top-k probabilities\n",
        "                # note: multinomial does not demand the input to sum to 1\n",
        "                ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
        "                # gather the corresponding indices\n",
        "                xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "                # append to the sequence\n",
        "                if TRAIN_SPACE:\n",
        "                    xcol = pack_token(xcol, space, upper)\n",
        "                xgen = torch.cat((xgen, xcol), dim=1)\n",
        "        # print the generated text\n",
        "        for i in range(num_return_sequences):\n",
        "            tokens = xgen[i, :max_length].tolist()\n",
        "            decoded = tokenizer.decode(tokens)\n",
        "            samples.append(decoded)\n",
        "            print(f\"rank {ddp_rank} sample {i}: {decoded}\")\n",
        "        samples = \"\\n\".join(samples)\n",
        "        run.log({\"step\": step, \"samples\": samples})\n",
        "\n",
        "    # do one step of the optimization\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum, loss_accum_ids, loss_accum_space, loss_accum_case = 0.0, 0.0, 0.0, 0.0\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        # added after video, this field is also used by the forward pass.\n",
        "        if ddp:\n",
        "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
        "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "            logits, losses = model(x, y)\n",
        "        # we have to scale the loss to account for gradient accumulation,\n",
        "        # because the gradients just add on each successive backward().\n",
        "        # addition of gradients corresponds to a SUM in the objective, but\n",
        "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
        "        (loss, loss_ids, loss_space, loss_case) = losses\n",
        "        loss_accum += (loss / grad_accum_steps).detach()\n",
        "        loss_accum_ids += (loss_ids / grad_accum_steps).detach()\n",
        "        loss_accum_space += (loss_space / grad_accum_steps).detach()\n",
        "        loss_accum_case += (loss_case / grad_accum_steps).detach()\n",
        "        loss = loss / grad_accum_steps\n",
        "        loss.backward()\n",
        "    if ddp:\n",
        "        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
        "        dist.all_reduce(loss_accum_ids, op=dist.ReduceOp.AVG)\n",
        "        dist.all_reduce(loss_accum_space, op=dist.ReduceOp.AVG)\n",
        "        dist.all_reduce(loss_accum_case, op=dist.ReduceOp.AVG)\n",
        "\n",
        "    norm_params = (param for name, param in model.named_parameters() if name not in ['emb_space.weight', 'emb_case.weight'])\n",
        "    norm = torch.nn.utils.clip_grad_norm_(norm_params, 1.0)\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    optimizer.step()\n",
        "    if device_type == \"cuda\":\n",
        "        torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0 # time difference in seconds\n",
        "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "    if master_process:\n",
        "        if step % 10 == 0:\n",
        "            print(f\"step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
        "        run.log({\"step\": step, \"loss\": loss_accum.item(), \"loss_ids\": loss_accum_ids.item(), \"loss_space\": loss_accum_space.item(), \"loss_case\": loss_accum_case.item(), \"lr\":lr, \"norm\":norm, \"dt\":1000*dt, \"tokens_per_sec\": tokens_per_sec })\n",
        "\n",
        "run.finish()\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
