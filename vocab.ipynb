{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install transformers numpy nltk ipywidgets tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk words corpus: 236736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('names')\n",
    "\n",
    "print(f\"nltk words corpus: {len(nltk.corpus.words.words())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               token     id\n",
      "0                  !      0\n",
      "1                  \"      1\n",
      "2                  #      2\n",
      "3                  $      3\n",
      "4                  %      4\n",
      "...              ...    ...\n",
      "50252       Ġregress  50252\n",
      "50253      ĠCollider  50253\n",
      "50254    Ġinformants  50254\n",
      "50255         Ġgazed  50255\n",
      "50256  <|endoftext|>  50256\n",
      "\n",
      "[50257 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "vocab_url = 'https://huggingface.co/openai-community/gpt2/raw/main/vocab.json'\n",
    "vocab_json_path = 'data/vocab.json'\n",
    "vocab_csv_path = 'data/vocab.csv'\n",
    "vocab_txt_path = 'data/vocab.txt'\n",
    "\n",
    "vocabUrlfile = urllib.request.urlopen(vocab_url).read()\n",
    "json_data = json.loads(vocabUrlfile)\n",
    "\n",
    "with open(vocab_json_path,'wb') as output:\n",
    "    output.write(vocabUrlfile)\n",
    "\n",
    "df = pd.DataFrame.from_dict(json_data, orient='index')\n",
    "df = df.reset_index()\n",
    "df.columns = ['token', 'id']\n",
    "df.to_csv(vocab_csv_path, index=False)\n",
    "\n",
    "df['token'].to_csv(vocab_txt_path, index=False, header=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isUncategorized.txt 9353\n",
      "isDigit.txt 1097\n",
      "isWord.txt 19817\n",
      "isInWordNet.txt 20128\n",
      "isName.txt 1676\n",
      "isNonAscii.txt 674\n",
      "isUncategorized.txt 9353\n",
      "words.txt 22917\n",
      "\n",
      "Dublicated count: 17313\n",
      "Uncategorized count: 11869\n"
     ]
    }
   ],
   "source": [
    "english_dictionary = set([word.lower() for word in nltk.corpus.words.words()])\n",
    "common_name_dictionary = set(nltk.corpus.names.words('male.txt')) | set(nltk.corpus.names.words('female.txt'))\n",
    "common_name_dictionary.update({item.lower() for item in common_name_dictionary})\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df['tokenCleaned'] = df['token'].str.replace(\"Ġ\", \"\").str.lower()\n",
    "\n",
    "df['isDigit'] = df['tokenCleaned'].str.isdigit()\n",
    "df['isWord'] = df['tokenCleaned'].map(lambda word: (lemmatizer.lemmatize(word, pos='n') in english_dictionary or lemmatizer.lemmatize(word, pos='v') in english_dictionary))\n",
    "df['isInWordNet'] = df['tokenCleaned'].map(lambda word: len(nltk.corpus.wordnet.synsets(word)) > 0)\n",
    "df['isName'] = df['tokenCleaned'].isin(common_name_dictionary)\n",
    "df['isNonAscii'] = ~df['tokenCleaned'].apply(lambda x: x.isascii())\n",
    "df['isUncategorized'] = ~(df['isDigit'] | df['isWord'] | df['isInWordNet'] | df['isName'] | df['isNonAscii'])\n",
    "df['isDuplicated'] = df.duplicated(subset='tokenCleaned', keep='first')\n",
    "\n",
    "\n",
    "df = df.sort_values(by='tokenCleaned', key=lambda x: x.str.len())\n",
    "\n",
    "def save_to_txt(df, category, filename, basePath=\"data/\"):\n",
    "    df_part = df[category & ~df['isDuplicated']]['tokenCleaned']\n",
    "    print(filename, len(df_part))\n",
    "    df_part.to_csv(basePath + filename, index=False, header=False)\n",
    "\n",
    "save_to_txt(df, df['isUncategorized'], 'isUncategorized.txt')\n",
    "save_to_txt(df, df['isDigit'], 'isDigit.txt')\n",
    "save_to_txt(df, df['isWord'], 'isWord.txt')\n",
    "save_to_txt(df, df['isInWordNet'], 'isInWordNet.txt')\n",
    "save_to_txt(df, df['isName'], 'isName.txt')\n",
    "save_to_txt(df, df['isNonAscii'], 'isNonAscii.txt')\n",
    "save_to_txt(df, df['isUncategorized'], 'isUncategorized.txt')\n",
    "save_to_txt(df, ~df['isUncategorized'] & ~df['isNonAscii'], 'words.txt')\n",
    "\n",
    "df.to_json('data/vocab-categorized.json', orient='records')\n",
    "\n",
    "print()\n",
    "print(\"Dublicated count:\", len(df[df['isDuplicated']]))\n",
    "print(\"Uncategorized count:\", len(df[df['isUncategorized']]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
